# -*- coding: utf-8 -*-
"""Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S5A0-kCRIVRQFu7UHwxlRM5RRRSS39xK
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, LeaveOneOut
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import zscore

# Read and inspect the dataset.

data = pd.read_csv("profit_prediction_regression.csv")

print("Dataset Info:")
print(data.info())

print("\nFirst 5 rows:")
print(data.head())

# Basic EDA

# Missing values
print("\nMissing Values:")
print(data.isnull().sum())

# Statistical summary (outlier detection)
print("\nStatistical Summary:")
print(data.describe())

# Missing Value Handing Using SimpleImputer

num_cols = data.select_dtypes(include=np.number).columns

simple_imputer = SimpleImputer(strategy='mean')
data[num_cols] = simple_imputer.fit_transform(data[num_cols])

# Missing Value Handing Using KNNImputer

# Uncomment to use KNN instead
# knn_imputer = KNNImputer(n_neighbors=5)
# data[num_cols] = knn_imputer.fit_transform(data[num_cols])

print("Shape before outlier removal:", data.shape)

#Outlier removal using Z score

z_scores = np.abs(zscore(data[num_cols]))
non_outliers = (z_scores < 3).all(axis=1)

data = data[non_outliers]

print("Shape after outlier removal:", data.shape)

# Train and Test Separate

X = data.drop('Profit', axis=1)
y = data['Profit']

# Encoding Categorical Column

X = pd.get_dummies(X, columns=['Area'], drop_first=True)



pd.get_dummies?

# Train Test Split Without CV

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Feature Scaling

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Linear Regression Without CV

lr = LinearRegression()
lr.fit(X_train_scaled, y_train)

y_pred_lr = lr.predict(X_test_scaled)

print("\nLinear Regression Performance:")
print("MSE:", mean_squared_error(y_test, y_pred_lr))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_lr)))
print("MAE:", mean_absolute_error(y_test, y_pred_lr))
print("RÂ²:", r2_score(y_test, y_pred_lr))

LinearRegression?

lr.coef_

# Compute correlation matrix
correlation_matrix = X.corr()

# Find features with correlation > 0.85
correlated_features = correlation_matrix[correlation_matrix > 0.85]
print(correlated_features)

Ridge?

# Ridge Without CV

alphas = [0.01, 0.1, 1, 10, 100]

# Initialize lists to store results
ridge_r2, lasso_r2 = [], []
ridge_rmse, lasso_rmse = [], []

#This line is split into two, to prevent a value error
ridge_mae = []
lasso_mae = []

ridge_coefficients, lasso_coefficients = [], []

# Train Ridge and Lasso models for different Î± values

for alpha in alphas:

    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train_scaled, y_train)
    ridge_pred = ridge.predict(X_test_scaled)
    ridge_r2.append(r2_score(y_test, ridge_pred))
    ridge_rmse.append(np.sqrt(mean_squared_error(y_test, ridge_pred)))
    ridge_mae.append(mean_absolute_error(y_test, ridge_pred))
    ridge_coefficients.append(ridge.coef_)

    # Lasso Without CV

    lasso = Lasso(alpha=alpha)
    lasso.fit(X_train_scaled, y_train)
    lasso_pred = lasso.predict(X_test_scaled)
    lasso_r2.append(r2_score(y_test, lasso_pred))
    lasso_rmse.append(np.sqrt(mean_squared_error(y_test, lasso_pred)))
    lasso_mae.append(mean_absolute_error(y_test, lasso_pred))
    lasso_coefficients.append(lasso.coef_)

print(ridge_r2) # print all of this

lasso_coefficients

# feature Selection with Lass -> No Need

#selected_features = X.columns[lasso.coef_ != 0]
#print("\nSelected Features by LASSO:")
#print(selected_features)

plt.figure(figsize=(14, 6))

# RÂ²
plt.subplot(1, 3, 1)
plt.plot(alphas, ridge_r2, marker='o', label='Ridge RÂ²')
plt.plot(alphas, lasso_r2, marker='o', label='Lasso RÂ²')
plt.xscale('log')
plt.xlabel('Alpha')
plt.ylabel('RÂ²')
plt.title('RÂ² vs Alpha')
plt.legend()

# RMSE
plt.subplot(1, 3, 2)
plt.plot(alphas, ridge_rmse, marker='o', label='Ridge RMSE')
plt.plot(alphas, lasso_rmse, marker='o', label='Lasso RMSE')
plt.xscale('log')
plt.xlabel('Alpha')
plt.ylabel('RMSE')
plt.title('RMSE vs Alpha')
plt.legend()

# MAE
plt.subplot(1, 3, 3)
plt.plot(alphas, ridge_mae, marker='o', label='Ridge MAE')
plt.plot(alphas, lasso_mae, marker='o', label='Lasso MAE')
plt.xscale('log')
plt.xlabel('Alpha')
plt.ylabel('MAE')
plt.title('MAE vs Alpha')
plt.legend()

plt.tight_layout()
plt.show()

# Co efficient path plot - Ridge

plt.figure(figsize=(12, 6))
plt.plot(alphas, ridge_coefficients, marker='o')
plt.xscale('log')
plt.xlabel('Alpha')
plt.ylabel('Coefficient Value')
plt.title('Ridge Coefficients vs Alpha')
plt.show()
#

# Co efficient Path Plot - Lasso

plt.figure(figsize=(12, 6))
plt.plot(alphas,  lasso_coefficients, marker='o')
plt.xscale('log')
plt.xlabel('Alpha')
plt.ylabel('Coefficient Value')
plt.title('Lasso Coefficients vs Alpha')
plt.show()

# Now using K Fold Cross Validation

kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Store RÂ² scores
r2_lr = []
r2_ridge = []
r2_lasso = []

for train_idx, test_idx in kf.split(X):
    X_train_cv, X_test_cv = X.iloc[train_idx], X.iloc[test_idx]
    y_train_cv, y_test_cv = y.iloc[train_idx], y.iloc[test_idx]

    # Scaling (correct place)
    X_train_cv_scaled = scaler.fit_transform(X_train_cv)
    X_test_cv_scaled = scaler.transform(X_test_cv)

    # ðŸ”¹ Linear Regression
    lr.fit(X_train_cv_scaled, y_train_cv)
    y_pred_lr = lr.predict(X_test_cv_scaled)
    r2_lr.append(r2_score(y_test_cv, y_pred_lr))

    # ðŸ”¹ Ridge Regression
    ridge.fit(X_train_cv_scaled, y_train_cv)
    y_pred_ridge = ridge.predict(X_test_cv_scaled)
    r2_ridge.append(r2_score(y_test_cv, y_pred_ridge))

    # ðŸ”¹ Lasso Regression
    lasso.fit(X_train_cv_scaled, y_train_cv)
    y_pred_lasso = lasso.predict(X_test_cv_scaled)
    r2_lasso.append(r2_score(y_test_cv, y_pred_lasso))

print("\nK-Fold Cross-Validation Accuracy (RÂ² Score):")

print(f"Linear Regression RÂ²: {np.mean(r2_lr)}")
print(f"Ridge Regression RÂ²: {np.mean(r2_ridge)}")
print(f"Lasso Regression RÂ²: {np.mean(r2_lasso)}")

folds = range(1, 6)

plt.figure(figsize=(14, 6))

# RÂ² per fold
plt.subplot(1, 2, 1)
plt.plot(folds, r2_lr, marker='o', label='Linear RÂ²')
plt.plot(folds, r2_ridge, marker='o', label='Ridge RÂ²')
plt.plot(folds, r2_lasso, marker='o', label='Lasso RÂ²')
plt.xlabel('Fold')
plt.ylabel('RÂ² Score')
plt.title('RÂ² Across Folds')
plt.legend()

# Mean RÂ² line
plt.axhline(y=np.mean(r2_lr), linestyle='--')

plt.tight_layout()
plt.show()

# using stratified K Fold

y_binned = pd.qcut(y, q=5, labels=False)

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

r2_lr = []
r2_ridge = []
r2_lasso = []

for train_idx, test_idx in skf.split(X, y_binned):
    X_train_cv, X_test_cv = X.iloc[train_idx], X.iloc[test_idx]
    y_train_cv, y_test_cv = y.iloc[train_idx], y.iloc[test_idx]

    # Scaling (correct)
    X_train_cv_scaled = scaler.fit_transform(X_train_cv)
    X_test_cv_scaled = scaler.transform(X_test_cv)

    # Linear Regression
    lr.fit(X_train_cv_scaled, y_train_cv)
    y_pred_lr = lr.predict(X_test_cv_scaled)
    r2_lr.append(r2_score(y_test_cv, y_pred_lr))

    # Ridge Regression
    ridge.fit(X_train_cv_scaled, y_train_cv)
    y_pred_ridge = ridge.predict(X_test_cv_scaled)
    r2_ridge.append(r2_score(y_test_cv, y_pred_ridge))

    # Lasso Regression
    lasso.fit(X_train_cv_scaled, y_train_cv)
    y_pred_lasso = lasso.predict(X_test_cv_scaled)
    r2_lasso.append(r2_score(y_test_cv, y_pred_lasso))

print("\nStratified K-Fold Cross-Validation Accuracy (RÂ²):")

print(f"Linear Regression RÂ²: {np.mean(r2_lr)}")
print(f"Ridge Regression RÂ²: {np.mean(r2_ridge)}")
print(f"Lasso Regression RÂ²: {np.mean(r2_lasso)}")

folds = range(1, len(r2_lr) + 1)

plt.figure(figsize=(14, 6))

# Plot RÂ² per fold
plt.plot(folds, r2_lr, marker='o', label='Linear Regression RÂ²')
plt.plot(folds, r2_ridge, marker='o', label='Ridge Regression RÂ²')
plt.plot(folds, r2_lasso, marker='o', label='Lasso Regression RÂ²')

# Mean RÂ² lines
plt.axhline(y=np.mean(r2_lr), linestyle='--', label='Mean Linear RÂ²')
plt.axhline(y=np.mean(r2_ridge), linestyle='--', label='Mean Ridge RÂ²')
plt.axhline(y=np.mean(r2_lasso), linestyle='--', label='Mean Lasso RÂ²')

plt.xlabel("Fold Number")
plt.ylabel("RÂ² Score")
plt.title("Stratified K-Fold Cross-Validation (RÂ² Score)")
plt.legend()
plt.grid(True)

plt.show()



